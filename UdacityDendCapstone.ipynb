{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering Capstone Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Explore and Assess the Data\n",
    "\n",
    "The purpose of this notebook is to read in the relevant data, and assess the following attributes of each data source;\n",
    "\n",
    "* Data schema.\n",
    "* Size of each data source.\n",
    "* Quality of each data source.\n",
    "\n",
    "As described in the README file, for each data source, we will read it into a data frame using pandas, and subsequently analyse the attributes. Pandas was chosen to read the data in such as to enable ease of use with airflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immigration Dataset\n",
    "\n",
    "The immigration dataset is stored in a series of parquet files. They are stored in `data/immigration-data/`. We are going to read them in using spark and analyse the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "## read in the parquet files from the directory\n",
    "data_directory = 'data/immigration-data'\n",
    "data_files = data_files = [os.path.join(data_directory, f) for f in os.listdir(data_directory)]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for f in data_files:\n",
    "    _df = pd.read_parquet(f)\n",
    "    dfs.append(_df)\n",
    "\n",
    "immigration_data = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'arrdate',\n",
       "       'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'count',\n",
       "       'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd', 'entdepu',\n",
       "       'matflag', 'biryear', 'dtaddto', 'gender', 'insnum', 'airline',\n",
       "       'admnum', 'fltno', 'visatype'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get the immigration data columns\n",
    "immigration_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3545479.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20563.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20583.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>10182016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>PR</td>\n",
       "      <td>9.399587e+10</td>\n",
       "      <td>00102</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3545480.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20563.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20583.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10182016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>PR</td>\n",
       "      <td>9.399437e+10</td>\n",
       "      <td>00152</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3545481.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20563.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20583.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>10182016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>PR</td>\n",
       "      <td>9.399443e+10</td>\n",
       "      <td>00152</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3545482.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20563.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20583.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>10182016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>PR</td>\n",
       "      <td>9.399598e+10</td>\n",
       "      <td>00102</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3545483.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20563.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20583.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>10182016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>PR</td>\n",
       "      <td>9.399603e+10</td>\n",
       "      <td>00102</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3545484.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20563.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20583.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>10182016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>PR</td>\n",
       "      <td>9.399590e+10</td>\n",
       "      <td>00102</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3545485.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20563.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20584.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>10182016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>PR</td>\n",
       "      <td>9.399643e+10</td>\n",
       "      <td>00102</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3545486.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20563.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20584.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>10182016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>CZ</td>\n",
       "      <td>9.399671e+10</td>\n",
       "      <td>00327</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3545487.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20563.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20585.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>10182016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>PR</td>\n",
       "      <td>9.399426e+10</td>\n",
       "      <td>00152</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3545488.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20563.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20585.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>10172016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>BR</td>\n",
       "      <td>9.392322e+10</td>\n",
       "      <td>00016</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode  \\\n",
       "0  3545479.0  2016.0     4.0   260.0   260.0     LOS  20563.0      1.0   \n",
       "1  3545480.0  2016.0     4.0   260.0   260.0     LOS  20563.0      1.0   \n",
       "2  3545481.0  2016.0     4.0   260.0   260.0     LOS  20563.0      1.0   \n",
       "3  3545482.0  2016.0     4.0   260.0   260.0     LOS  20563.0      1.0   \n",
       "4  3545483.0  2016.0     4.0   260.0   260.0     LOS  20563.0      1.0   \n",
       "5  3545484.0  2016.0     4.0   260.0   260.0     LOS  20563.0      1.0   \n",
       "6  3545485.0  2016.0     4.0   260.0   260.0     LOS  20563.0      1.0   \n",
       "7  3545486.0  2016.0     4.0   260.0   260.0     LOS  20563.0      1.0   \n",
       "8  3545487.0  2016.0     4.0   260.0   260.0     LOS  20563.0      1.0   \n",
       "9  3545488.0  2016.0     4.0   260.0   260.0     LOS  20563.0      1.0   \n",
       "\n",
       "  i94addr  depdate  ...  entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      CA  20583.0  ...     None        M   1968.0  10182016      M   None   \n",
       "1      CA  20583.0  ...     None        M   1979.0  10182016      M   None   \n",
       "2      CA  20583.0  ...     None        M   1980.0  10182016      F   None   \n",
       "3      CA  20583.0  ...     None        M   1998.0  10182016      F   None   \n",
       "4      CA  20583.0  ...     None        M   1999.0  10182016      M   None   \n",
       "5      CA  20583.0  ...     None        M   2007.0  10182016      F   None   \n",
       "6      CA  20584.0  ...     None        M   1962.0  10182016      F   None   \n",
       "7      CA  20584.0  ...     None        M   1970.0  10182016      F   None   \n",
       "8      CA  20585.0  ...     None        M   1957.0  10182016      M   None   \n",
       "9      CA  20585.0  ...     None        M   1962.0  10172016      M   None   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0      PR  9.399587e+10  00102       B2  \n",
       "1      PR  9.399437e+10  00152       B2  \n",
       "2      PR  9.399443e+10  00152       B2  \n",
       "3      PR  9.399598e+10  00102       B2  \n",
       "4      PR  9.399603e+10  00102       B2  \n",
       "5      PR  9.399590e+10  00102       B2  \n",
       "6      PR  9.399643e+10  00102       B2  \n",
       "7      CZ  9.399671e+10  00327       B2  \n",
       "8      PR  9.399426e+10  00152       B2  \n",
       "9      BR  9.392322e+10  00016       B2  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get the first 10 rows\n",
    "immigration_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    arrdate  depdate   dtaddto\n",
      "0   20563.0  20583.0  10182016\n",
      "1   20563.0  20583.0  10182016\n",
      "2   20563.0  20583.0  10182016\n",
      "3   20563.0  20583.0  10182016\n",
      "4   20563.0  20583.0  10182016\n",
      "5   20563.0  20583.0  10182016\n",
      "6   20563.0  20584.0  10182016\n",
      "7   20563.0  20584.0  10182016\n",
      "8   20563.0  20585.0  10182016\n",
      "9   20563.0  20585.0  10172016\n",
      "10  20563.0  20585.0  10172016\n",
      "11  20563.0  20585.0  10172016\n",
      "12  20563.0  20585.0  10172016\n",
      "13  20563.0  20586.0  10182016\n",
      "14  20563.0  20586.0  10182016\n",
      "15  20563.0  20586.0  10182016\n",
      "16  20563.0  20586.0  10182016\n",
      "17  20563.0  20586.0  10182016\n",
      "18  20563.0  20586.0  10182016\n",
      "19  20563.0  20586.0  10182016\n",
      "20  20563.0  20586.0  10182016\n",
      "21  20563.0  20587.0  10182016\n",
      "22  20563.0  20587.0  10182016\n",
      "23  20563.0  20587.0  10182016\n",
      "24  20563.0  20587.0  10182016\n",
      "25  20563.0  20587.0  10182016\n",
      "26  20563.0  20588.0  10182016\n",
      "27  20563.0  20588.0  10182016\n",
      "28  20563.0  20588.0  10182016\n",
      "29  20563.0  20589.0  10182016\n",
      "30  20563.0  20589.0  10182016\n",
      "31  20563.0  20589.0  10182016\n",
      "32  20563.0  20589.0  10182016\n",
      "33  20563.0  20590.0  10182016\n",
      "34  20563.0  20590.0  10182016\n",
      "35  20563.0  20592.0  10182016\n",
      "36  20563.0  20592.0  10182016\n",
      "37  20563.0  20592.0  10182016\n",
      "38  20563.0  20592.0  10182016\n",
      "39  20563.0  20593.0  10182016\n",
      "40  20563.0  20593.0  10182016\n",
      "41  20563.0  20593.0  10182016\n",
      "42  20563.0  20593.0  10182016\n",
      "43  20563.0  20593.0  10182016\n",
      "44  20563.0  20594.0  10182016\n",
      "45  20563.0  20594.0  10182016\n",
      "46  20563.0  20594.0  10182016\n",
      "47  20563.0  20594.0  10182016\n",
      "48  20563.0  20594.0  10182016\n",
      "49  20563.0  20594.0  10182016\n"
     ]
    }
   ],
   "source": [
    "## analyse the timestamp rows\n",
    "def show_ts_columns(df):\n",
    "    print(df[['arrdate', 'depdate', 'dtaddto']].head(50))\n",
    "    \n",
    "show_ts_columns(immigration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert sas timestamp to date\n",
    "def convert_sas_timestamp(column_name, df):\n",
    "    df[column_name] = pd.to_timedelta(df[column_name], unit='D') + pd.Timestamp('1960-1-1')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      arrdate    depdate   dtaddto\n",
      "0  2016-04-19 2016-05-09  10182016\n",
      "1  2016-04-19 2016-05-09  10182016\n",
      "2  2016-04-19 2016-05-09  10182016\n",
      "3  2016-04-19 2016-05-09  10182016\n",
      "4  2016-04-19 2016-05-09  10182016\n",
      "5  2016-04-19 2016-05-09  10182016\n",
      "6  2016-04-19 2016-05-10  10182016\n",
      "7  2016-04-19 2016-05-10  10182016\n",
      "8  2016-04-19 2016-05-11  10182016\n",
      "9  2016-04-19 2016-05-11  10172016\n",
      "10 2016-04-19 2016-05-11  10172016\n",
      "11 2016-04-19 2016-05-11  10172016\n",
      "12 2016-04-19 2016-05-11  10172016\n",
      "13 2016-04-19 2016-05-12  10182016\n",
      "14 2016-04-19 2016-05-12  10182016\n",
      "15 2016-04-19 2016-05-12  10182016\n",
      "16 2016-04-19 2016-05-12  10182016\n",
      "17 2016-04-19 2016-05-12  10182016\n",
      "18 2016-04-19 2016-05-12  10182016\n",
      "19 2016-04-19 2016-05-12  10182016\n",
      "20 2016-04-19 2016-05-12  10182016\n",
      "21 2016-04-19 2016-05-13  10182016\n",
      "22 2016-04-19 2016-05-13  10182016\n",
      "23 2016-04-19 2016-05-13  10182016\n",
      "24 2016-04-19 2016-05-13  10182016\n",
      "25 2016-04-19 2016-05-13  10182016\n",
      "26 2016-04-19 2016-05-14  10182016\n",
      "27 2016-04-19 2016-05-14  10182016\n",
      "28 2016-04-19 2016-05-14  10182016\n",
      "29 2016-04-19 2016-05-15  10182016\n",
      "30 2016-04-19 2016-05-15  10182016\n",
      "31 2016-04-19 2016-05-15  10182016\n",
      "32 2016-04-19 2016-05-15  10182016\n",
      "33 2016-04-19 2016-05-16  10182016\n",
      "34 2016-04-19 2016-05-16  10182016\n",
      "35 2016-04-19 2016-05-18  10182016\n",
      "36 2016-04-19 2016-05-18  10182016\n",
      "37 2016-04-19 2016-05-18  10182016\n",
      "38 2016-04-19 2016-05-18  10182016\n",
      "39 2016-04-19 2016-05-19  10182016\n",
      "40 2016-04-19 2016-05-19  10182016\n",
      "41 2016-04-19 2016-05-19  10182016\n",
      "42 2016-04-19 2016-05-19  10182016\n",
      "43 2016-04-19 2016-05-19  10182016\n",
      "44 2016-04-19 2016-05-20  10182016\n",
      "45 2016-04-19 2016-05-20  10182016\n",
      "46 2016-04-19 2016-05-20  10182016\n",
      "47 2016-04-19 2016-05-20  10182016\n",
      "48 2016-04-19 2016-05-20  10182016\n",
      "49 2016-04-19 2016-05-20  10182016\n"
     ]
    }
   ],
   "source": [
    "## convert arrival date and departure dates \n",
    "immigration_data = convert_sas_timestamp('arrdate', immigration_data)\n",
    "immigration_data = convert_sas_timestamp('depdate', immigration_data)\n",
    "\n",
    "show_ts_columns(immigration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter where the immigration departure is valid\n",
    "immigration_data = immigration_data[immigration_data['dtaddto'].str.len() == 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the datetime column\n",
    "immigration_data['dtaddto'] = pd.to_datetime(immigration_data['dtaddto'], format=\"%m%d%Y\", errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      arrdate    depdate    dtaddto\n",
      "0  2016-04-19 2016-05-09 2016-10-18\n",
      "1  2016-04-19 2016-05-09 2016-10-18\n",
      "2  2016-04-19 2016-05-09 2016-10-18\n",
      "3  2016-04-19 2016-05-09 2016-10-18\n",
      "4  2016-04-19 2016-05-09 2016-10-18\n",
      "5  2016-04-19 2016-05-09 2016-10-18\n",
      "6  2016-04-19 2016-05-10 2016-10-18\n",
      "7  2016-04-19 2016-05-10 2016-10-18\n",
      "8  2016-04-19 2016-05-11 2016-10-18\n",
      "9  2016-04-19 2016-05-11 2016-10-17\n",
      "10 2016-04-19 2016-05-11 2016-10-17\n",
      "11 2016-04-19 2016-05-11 2016-10-17\n",
      "12 2016-04-19 2016-05-11 2016-10-17\n",
      "13 2016-04-19 2016-05-12 2016-10-18\n",
      "14 2016-04-19 2016-05-12 2016-10-18\n",
      "15 2016-04-19 2016-05-12 2016-10-18\n",
      "16 2016-04-19 2016-05-12 2016-10-18\n",
      "17 2016-04-19 2016-05-12 2016-10-18\n",
      "18 2016-04-19 2016-05-12 2016-10-18\n",
      "19 2016-04-19 2016-05-12 2016-10-18\n",
      "20 2016-04-19 2016-05-12 2016-10-18\n",
      "21 2016-04-19 2016-05-13 2016-10-18\n",
      "22 2016-04-19 2016-05-13 2016-10-18\n",
      "23 2016-04-19 2016-05-13 2016-10-18\n",
      "24 2016-04-19 2016-05-13 2016-10-18\n",
      "25 2016-04-19 2016-05-13 2016-10-18\n",
      "26 2016-04-19 2016-05-14 2016-10-18\n",
      "27 2016-04-19 2016-05-14 2016-10-18\n",
      "28 2016-04-19 2016-05-14 2016-10-18\n",
      "29 2016-04-19 2016-05-15 2016-10-18\n",
      "30 2016-04-19 2016-05-15 2016-10-18\n",
      "31 2016-04-19 2016-05-15 2016-10-18\n",
      "32 2016-04-19 2016-05-15 2016-10-18\n",
      "33 2016-04-19 2016-05-16 2016-10-18\n",
      "34 2016-04-19 2016-05-16 2016-10-18\n",
      "35 2016-04-19 2016-05-18 2016-10-18\n",
      "36 2016-04-19 2016-05-18 2016-10-18\n",
      "37 2016-04-19 2016-05-18 2016-10-18\n",
      "38 2016-04-19 2016-05-18 2016-10-18\n",
      "39 2016-04-19 2016-05-19 2016-10-18\n",
      "40 2016-04-19 2016-05-19 2016-10-18\n",
      "41 2016-04-19 2016-05-19 2016-10-18\n",
      "42 2016-04-19 2016-05-19 2016-10-18\n",
      "43 2016-04-19 2016-05-19 2016-10-18\n",
      "44 2016-04-19 2016-05-20 2016-10-18\n",
      "45 2016-04-19 2016-05-20 2016-10-18\n",
      "46 2016-04-19 2016-05-20 2016-10-18\n",
      "47 2016-04-19 2016-05-20 2016-10-18\n",
      "48 2016-04-19 2016-05-20 2016-10-18\n",
      "49 2016-04-19 2016-05-20 2016-10-18\n"
     ]
    }
   ],
   "source": [
    "show_ts_columns(immigration_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature Data\n",
    "\n",
    "The temperature data is divided into four csv files;\n",
    "\n",
    "* GlobalTemperatures.csv\n",
    "* GlobalLandTemperaturesByCity.csv\n",
    "* GlobalLandTemperaturesByCountry.csv\n",
    "* GlobalLandTemperaturesByMajorCity.csv\n",
    "* GlobalLandTemperaturesByState.csv\n",
    "\n",
    "For each of the csv files, we will read them in using pandas, we will get the schema, print the first 10 rows of the data, and display the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Analysing Data Source:: GlobalTemperatures :: File Path :: ./data/climate-change/GlobalTemperatures.csv ==\n",
      "== Analysing Data Source:: GlobalLandTemperaturesByCountry :: File Path :: ./data/climate-change/GlobalLandTemperaturesByCountry.csv ==\n",
      "\n",
      "** SCHEMA **\n",
      "\n",
      "['dt', 'AverageTemperature', 'AverageTemperatureUncertainty', 'Country']\n",
      "\n",
      "Before::Index(['dt', 'AverageTemperature', 'AverageTemperatureUncertainty', 'Country'], dtype='object')\n",
      "After::Index(['dt', 'AverageTemperature', 'Country'], dtype='object')\n",
      "\n",
      "** FIRST 10 ROWS **\n",
      "\n",
      "           ts  average_temperature country_code\n",
      "0  1743-11-01                4.384        Åland\n",
      "1  1743-12-01                  NaN        Åland\n",
      "2  1744-01-01                  NaN        Åland\n",
      "3  1744-02-01                  NaN        Åland\n",
      "4  1744-03-01                  NaN        Åland\n",
      "5  1744-04-01                1.530        Åland\n",
      "6  1744-05-01                6.702        Åland\n",
      "7  1744-06-01               11.609        Åland\n",
      "8  1744-07-01               15.342        Åland\n",
      "9  1744-08-01                  NaN        Åland\n",
      "\n",
      "\n",
      "** NUMBER OF ROWS **\n",
      "\n",
      "577462\n",
      "\n",
      "== Analysing Data Source:: GlobalLandTemperaturesByMajorCity :: File Path :: ./data/climate-change/GlobalLandTemperaturesByMajorCity.csv ==\n",
      "== Analysing Data Source:: GlobalLandTemperaturesByState :: File Path :: ./data/climate-change/GlobalLandTemperaturesByState.csv ==\n"
     ]
    }
   ],
   "source": [
    "## base path for the csv files\n",
    "base_path = './data/climate-change'\n",
    "\n",
    "## list of the files\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_names = ['GlobalTemperatures', \n",
    "#               'GlobalLandTemperaturesByCity', \n",
    "              'GlobalLandTemperaturesByCountry',\n",
    "              'GlobalLandTemperaturesByMajorCity',\n",
    "              'GlobalLandTemperaturesByState']\n",
    "\n",
    "for data_source in file_names:\n",
    "    data_dest = os.path.join(base_path, f'{data_source}.csv')\n",
    "    print(f'== Analysing Data Source:: {data_source} :: File Path :: {data_dest} ==')\n",
    "          \n",
    "    data_df = pd.read_csv(data_dest)\n",
    "    \n",
    "    if data_source == 'GlobalLandTemperaturesByCountry':\n",
    "        \n",
    "        ## print the schema\n",
    "        print('\\n** SCHEMA **\\n')\n",
    "        print(list(data_df))\n",
    "        print()\n",
    "        \n",
    "        print(f'Before::{data_df.columns}')\n",
    "        \n",
    "        data_df.drop(['AverageTemperatureUncertainty'], inplace=True, axis=1)\n",
    "\n",
    "        print(f'After::{data_df.columns}')\n",
    "        \n",
    "        data_df.columns = ['ts', \n",
    "                           'average_temperature',\n",
    "                           'country_code']\n",
    "        \n",
    "        ## get the first 10 rows\n",
    "        print('\\n** FIRST 10 ROWS **\\n')\n",
    "        print(data_df.head(10))\n",
    "        print()\n",
    "\n",
    "        ## get the count\n",
    "        print('\\n** NUMBER OF ROWS **\\n')\n",
    "        print(len(data_df))\n",
    "        print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographics\n",
    "\n",
    "The demographics dataset contains information about the demographics of all US cities. We will read in the csv files using pandas and get the schema, first 10 rows, and the row count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** SCHEMA **\n",
      "\n",
      "['City', 'State', 'Median Age', 'Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born', 'Average Household Size', 'State Code', 'Race', 'Count']\n",
      "\n",
      "Before::Index(['City', 'State', 'Median Age', 'Male Population', 'Female Population',\n",
      "       'Total Population', 'Number of Veterans', 'Foreign-born',\n",
      "       'Average Household Size', 'State Code', 'Race', 'Count'],\n",
      "      dtype='object')\n",
      "NewColumns::['city', 'state', 'median_age', 'male_population', 'female_population', 'total_population', 'number_of_veterans', 'foreign_born', 'average_household_size', 'state_code', 'race', 'count']::12\n",
      "After::Index(['city', 'state', 'median_age', 'male_population', 'female_population',\n",
      "       'total_population', 'number_of_veterans', 'foreign_born',\n",
      "       'average_household_size', 'state_code', 'race', 'count'],\n",
      "      dtype='object')\n",
      "\n",
      "** FIRST 10 ROWS **\n",
      "\n",
      "             city           state  median_age  male_population  \\\n",
      "0     Los Angeles      California        35.0        1958998.0   \n",
      "1        Metairie       Louisiana        41.6          69515.0   \n",
      "2      Boca Raton         Florida        47.3          44760.0   \n",
      "3          Quincy   Massachusetts        41.0          44129.0   \n",
      "4      Union City      California        38.5          38599.0   \n",
      "5        Lakeland         Florida        38.1          47840.0   \n",
      "6         Antioch      California        34.0          54733.0   \n",
      "7  Mount Pleasant  South Carolina        42.7          39429.0   \n",
      "8      Des Moines            Iowa        34.5         103726.0   \n",
      "9   Jurupa Valley      California        33.8          49430.0   \n",
      "\n",
      "   female_population  total_population  number_of_veterans  foreign_born  \\\n",
      "0          2012898.0           3971896             85417.0     1485425.0   \n",
      "1            76943.0            146458              7187.0       19871.0   \n",
      "2            48466.0             93226              4367.0       21117.0   \n",
      "3            49500.0             93629              4147.0       32935.0   \n",
      "4            35911.0             74510              1440.0       32752.0   \n",
      "5            56570.0            104410              7390.0       11592.0   \n",
      "6            55809.0            110542              5681.0       24942.0   \n",
      "7            41880.0             81309              3981.0        4701.0   \n",
      "8           106591.0            210317             11780.0       23857.0   \n",
      "9            50884.0            100314              3833.0       25338.0   \n",
      "\n",
      "   average_household_size state_code                       race    count  \n",
      "0                    2.86         CA                      White  2177650  \n",
      "1                    2.39         LA                      White   124270  \n",
      "2                    2.22         FL                      White    80781  \n",
      "3                    2.39         MA         Hispanic or Latino     2566  \n",
      "4                    3.46         CA                      White    16845  \n",
      "5                    2.56         FL  Black or African-American    25111  \n",
      "6                    3.31         CA         Hispanic or Latino    35563  \n",
      "7                    2.53         SC                      Asian     1827  \n",
      "8                    2.41         IA         Hispanic or Latino    25306  \n",
      "9                    3.87         CA                      Asian     4992  \n",
      "\n",
      "\n",
      "** ROW COUNT **\n",
      "\n",
      "2891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = './data/demographics/us-cities-demographics.csv'\n",
    "\n",
    "demographics_df = pd.read_csv(file_path, delimiter=\";\")\n",
    "\n",
    "## get the schema\n",
    "print('\\n** SCHEMA **\\n')\n",
    "print(list(demographics_df))\n",
    "print()\n",
    "\n",
    "## get the columns\n",
    "\n",
    "print(f'Before::{demographics_df.columns}')\n",
    "\n",
    "df_cols = ['city',\n",
    "           'state',\n",
    "           'median_age',\n",
    "           'male_population',\n",
    "           'female_population',\n",
    "           'total_population',\n",
    "           'number_of_veterans',\n",
    "           'foreign_born',\n",
    "           'average_household_size',\n",
    "           'state_code',\n",
    "           'race',\n",
    "           'count']\n",
    "\n",
    "print(f'NewColumns::{df_cols}::{len(df_cols)}')\n",
    "\n",
    "demographics_df.columns = df_cols\n",
    "\n",
    "print(f'After::{demographics_df.columns}')\n",
    "\n",
    "## get the first 10 rows\n",
    "print('\\n** FIRST 10 ROWS **\\n')\n",
    "print(demographics_df.head(10))\n",
    "print()\n",
    "\n",
    "## get the row count\n",
    "print('\\n** ROW COUNT **\\n')\n",
    "print(len(demographics_df))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airport Codes\n",
    "\n",
    "The airport codes dataset contains airport codes, and corresponding cities\n",
    "\n",
    "We will read in the `.csv` file using pandas, get the schema, the first 10 rows, and the length of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/airport-codes/airport-codes_csv.csv'\n",
    "\n",
    "airport_codes_df = pd.read_csv(file_path)\n",
    "\n",
    "print('\\n** SCHEMA **\\n')\n",
    "print(list(airport_codes_df))\n",
    "print()\n",
    "\n",
    "print('\\n** FIRST 10 ROWS **\\n')\n",
    "print(airport_codes_df.head(10))\n",
    "print()\n",
    "\n",
    "print('\\n** ROW COUNT **\\n')\n",
    "print(len(airport_codes_df))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
