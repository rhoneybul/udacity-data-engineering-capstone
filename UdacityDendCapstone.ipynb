{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering Capstone Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Explore and Assess the Data\n",
    "\n",
    "The purpose of this notebook is to read in the relevant data, and assess the following attributes of each data source;\n",
    "\n",
    "* Data schema.\n",
    "* Size of each data source.\n",
    "* Quality of each data source.\n",
    "\n",
    "As described in the README file, for each data source, we will read it into a data frame using pandas, and subsequently analyse the attributes. Pandas was chosen to read the data in such as to enable ease of use with airflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immigration Dataset\n",
    "\n",
    "The immigration dataset is stored in a series of parquet files. They are stored in `data/immigration-data/`. We are going to read them in using spark and analyse the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "## read in the parquet files from the directory\n",
    "data_directory = 'data/immigration-data'\n",
    "data_files = data_files = [os.path.join(data_directory, f) for f in os.listdir(data_directory)]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for f in data_files:\n",
    "    _df = pd.read_parquet(f)\n",
    "    dfs.append(_df)\n",
    "\n",
    "immigration_data = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the immigration data columns\n",
    "immigration_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the first 10 rows\n",
    "immigration_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the full length\n",
    "len(immigration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_data = immigration_data.drop_duplicates('admnum')\n",
    "len(immigration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## analyse the timestamp rows\n",
    "def show_ts_columns(df):\n",
    "    print(df[['arrdate', 'depdate', 'dtaddto']].head(50))\n",
    "    \n",
    "show_ts_columns(immigration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert sas timestamp to date\n",
    "def convert_sas_timestamp(column_name, df):\n",
    "    df[column_name] = pd.to_timedelta(df[column_name], unit='D') + pd.Timestamp('1960-1-1')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert arrival date and departure dates \n",
    "immigration_data = convert_sas_timestamp('arrdate', immigration_data)\n",
    "immigration_data = convert_sas_timestamp('depdate', immigration_data)\n",
    "\n",
    "show_ts_columns(immigration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter where the immigration departure is valid\n",
    "immigration_data = immigration_data[immigration_data['dtaddto'].str.len() == 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the datetime column\n",
    "immigration_data['dtaddto'] = pd.to_datetime(immigration_data['dtaddto'], format=\"%m%d%Y\", errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ts_columns(immigration_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature Data\n",
    "\n",
    "The temperature data is divided into four csv files;\n",
    "\n",
    "* GlobalTemperatures.csv\n",
    "* GlobalLandTemperaturesByCity.csv\n",
    "* GlobalLandTemperaturesByCountry.csv\n",
    "* GlobalLandTemperaturesByMajorCity.csv\n",
    "* GlobalLandTemperaturesByState.csv\n",
    "\n",
    "For each of the csv files, we will read them in using pandas, we will get the schema, print the first 10 rows of the data, and display the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## base path for the csv files\n",
    "base_path = './data/climate-change'\n",
    "\n",
    "## list of the files\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_names = ['GlobalTemperatures', \n",
    "#               'GlobalLandTemperaturesByCity', \n",
    "              'GlobalLandTemperaturesByCountry',\n",
    "              'GlobalLandTemperaturesByMajorCity',\n",
    "              'GlobalLandTemperaturesByState']\n",
    "\n",
    "for data_source in file_names:\n",
    "    data_dest = os.path.join(base_path, f'{data_source}.csv')\n",
    "    print(f'== Analysing Data Source:: {data_source} :: File Path :: {data_dest} ==')\n",
    "          \n",
    "    data_df = pd.read_csv(data_dest)\n",
    "            \n",
    "    ## print the schema\n",
    "    print('\\n** SCHEMA **\\n')\n",
    "    print(list(data_df))\n",
    "    print()\n",
    "\n",
    "    data_df.columns = ['ts', \n",
    "                       'average_temperature',\n",
    "                       'country_code']\n",
    "\n",
    "    ## get the first 10 rows\n",
    "    print('\\n** FIRST 10 ROWS **\\n')\n",
    "    print(data_df.head(10))\n",
    "    print()\n",
    "\n",
    "    ## get the count\n",
    "    print('\\n** NUMBER OF ROWS **\\n')\n",
    "    print(len(data_df))\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographics\n",
    "\n",
    "The demographics dataset contains information about the demographics of all US cities. We will read in the csv files using pandas and get the schema, first 10 rows, and the row count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/demographics/us-cities-demographics.csv'\n",
    "\n",
    "demographics_df = pd.read_csv(file_path, delimiter=\";\")\n",
    "\n",
    "## get the schema\n",
    "print('\\n** SCHEMA **\\n')\n",
    "print(list(demographics_df))\n",
    "print()\n",
    "\n",
    "## get the columns\n",
    "\n",
    "print(f'Before::{demographics_df.columns}')\n",
    "\n",
    "df_cols = ['city',\n",
    "           'state',\n",
    "           'median_age',\n",
    "           'male_population',\n",
    "           'female_population',\n",
    "           'total_population',\n",
    "           'number_of_veterans',\n",
    "           'foreign_born',\n",
    "           'average_household_size',\n",
    "           'state_code',\n",
    "           'race',\n",
    "           'count']\n",
    "\n",
    "print(f'NewColumns::{df_cols}::{len(df_cols)}')\n",
    "\n",
    "demographics_df.columns = df_cols\n",
    "\n",
    "print(f'After::{demographics_df.columns}')\n",
    "\n",
    "## get the first 10 rows\n",
    "print('\\n** FIRST 10 ROWS **\\n')\n",
    "print(demographics_df.head(10))\n",
    "print()\n",
    "\n",
    "## get the row count\n",
    "print('\\n** ROW COUNT **\\n')\n",
    "print(len(demographics_df))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airport Codes\n",
    "\n",
    "The airport codes dataset contains airport codes, and corresponding cities\n",
    "\n",
    "We will read in the `.csv` file using pandas, get the schema, the first 10 rows, and the length of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/airport-codes/airport-codes_csv.csv'\n",
    "\n",
    "airport_codes_df = pd.read_csv(file_path)\n",
    "\n",
    "print('\\n** SCHEMA **\\n')\n",
    "print(list(airport_codes_df))\n",
    "print()\n",
    "\n",
    "print('\\n** FIRST 10 ROWS **\\n')\n",
    "print(airport_codes_df.head(10))\n",
    "print()\n",
    "\n",
    "print('\\n** ROW COUNT **\\n')\n",
    "print(len(airport_codes_df))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
